services:
  # This service is ONLY for generating the dataset ONCE.
#  dataset-builder:
#    build: .
    # Mount the data volume to this service
#    volumes:
#      - ./docker-data:/app/data
    # This command will run and then the container will exit.
#    command: >
#      python dataset/build_sudoku_dataset.py
#      --output-dir data/sudoku-extreme-1k-aug-1000
#      --subsample-size 1000
#      --num-aug 1000

  # This is your main training service.
  hrm-trainer:
    image: hrm
    # This service depends on the dataset-builder having finished successfully.
#    depends_on:
#      - dataset-builder
    environment:
      - WANDB_API_KEY=${WANDB_API_KEY}
      - OMP_NUM_THREADS=8
    volumes:
      - ./checkpoints:/app/checkpoints

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # 2. Override the CMD to run training with a sensible batch size
    command: python3 pretrain.py data_path=data/sudoku-extreme-1k-aug-1000 epochs=20000 eval_interval=2000 global_batch_size=384 lr=7e-5 puzzle_emb_lr=7e-5 weight_decay=1.0 puzzle_emb_weight_decay=1.0
